# zhihu
本项目使用多线程、多进程及生产者消费者模型爬取知乎上部分话题下的所有问题的标题、url。
大致思路如下：
zhihu1:  1、在话题广场爬取33类父话题的url
         2、使用10个线程分别爬取33类父话题内的子话题url，并去重
         3、使用10个线程分别爬取以上子话题内的子话题url，并去重
         4、把所有子话题url中的id存储到Mysql
     
23508条url爬取加去重耗时10min，存储耗时53min。

zhihu2:  1、创建队列，生产者读取url的id并将生产的url放入队列
         2、使用5个进程，消费者从队列中取出url进行爬取（由于话题中的问题是动态加载的，从队列中取到的url只能爬取到第0-5个问题的数据以及存放第6-10个问题的url，而存放第6-10个问题的url只能爬取到第6-10个问题的数据以及存放第11-15个问题的url，以此类推，所以消费者实际上需要边爬数据边请求新的url，直到没有新的url为止才算爬完了一个话题下的所有数据。）
         3、将爬取到的问题数据（话题、标题、url）存储到Mysql

211462条数据（（topic,title,url）算一条），32.56MB，爬取加存储耗时7.7h

遇到的问题：想爬取知乎全站数据很难，一是访问过多url会出现验证码，使用验证码识别必然影响爬取速度且识别度无法保证，另外验证成功后仍可能丢失当前正在爬取的url；二是使用代理，想要逃过验证码，必然需要大量的优质ip，而这样的ip不好找。
